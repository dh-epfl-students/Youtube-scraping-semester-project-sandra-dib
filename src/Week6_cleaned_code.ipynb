{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping \n",
    "\n",
    "Goal : get a table of all the locations listed [here](https://en.wikipedia.org/wiki/Lists_of_World_Heritage_Sites), in the format \\[url; name; coordinates\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pandas as pd\n",
    "import lxml\n",
    "import unicodedata2 as unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url=\"https://en.wikipedia.org/wiki/Lists_of_World_Heritage_Sites\")\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "title = soup.find(id=\"firstHeading\")\n",
    "\n",
    "# Get all the links\n",
    "allLinks = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "\n",
    "\n",
    "link_list = []\n",
    "str_link = str(allLinks).split('<a')\n",
    "\n",
    "for link in str_link:\n",
    "    if 'href=\"/wiki/' in link:\n",
    "        link_list.append(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links1 = []\n",
    "for elem in soup.select(\"a[href*=wiki\\/List_of]\"):\n",
    "    links1.append(elem['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file\n",
    "with open('../data/unesco_sites_raw.csv', 'w') as f:\n",
    "\n",
    "    for link in links1:\n",
    "        response = requests.get(url = 'https://en.wikipedia.org' + link)\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table',{'class':\"wikitable\"})\n",
    "        df = pd.read_html(str(table))\n",
    "\n",
    "        # convert list to dataframe\n",
    "        df = pd.DataFrame(df[0])\n",
    "        newdf = df.fillna('')\n",
    "\n",
    "        # drop the unwanted column\n",
    "        if 'Site' in newdf.columns and 'Location' in newdf.columns:\n",
    "            data = newdf[['Site', 'Location']]\n",
    "\n",
    "            # add information to the csv file\n",
    "            for index, value in data.iterrows():\n",
    "                location_cleaned = unicodedata.normalize('NFKD', value['Location']).encode('ascii','ignore').decode('ascii').replace(';', ',')\n",
    "                f.write(\"%s;%s;%s\\n\" % (link, value['Site'], location_cleaned))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data \n",
    "Clean the text related to place entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from tqdm.autonotebook import tqdm\n",
    "from Levenshtein import distance as lev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/unesco_sites_raw.csv', delimiter=';', header = None)\n",
    "data.rename(columns = {0: 'link', 1: 'place', 2: 'location'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['place'] = data['place'].apply(lambda x: x.replace(\"-\",\" \"))\n",
    "data['place'] = data['place'].apply(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', x))\n",
    "data['place'] = data['place'].apply(lambda x: x.strip())\n",
    "data['place'] = data['place'].apply(lambda x: x.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3810, 1190)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(data.place.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>place</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/wiki/List_of_World_Heritage_Sites_in_Africa</td>\n",
       "      <td>Aapravasi ghat</td>\n",
       "      <td>Port Louis District, Mauritius.mw-parser-outpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/wiki/List_of_World_Heritage_Sites_in_Africa</td>\n",
       "      <td>Abu mena</td>\n",
       "      <td>Abusir, Egypt305028N 293947E / 30.84098N 29.66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/wiki/List_of_World_Heritage_Sites_in_Africa</td>\n",
       "      <td>Air and tnr natural reserves</td>\n",
       "      <td>Arlit Department, Niger18N 9E / 18N 9E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/wiki/List_of_World_Heritage_Sites_in_Africa</td>\n",
       "      <td>Aksum</td>\n",
       "      <td>Tigray Region, Ethiopia140749N 384307E / 14.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/wiki/List_of_World_Heritage_Sites_in_Africa</td>\n",
       "      <td>Al qala of beni hammad</td>\n",
       "      <td>Maadid, Algeria354906N 44713E / 35.818440N 4.7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           link                         place  \\\n",
       "0  /wiki/List_of_World_Heritage_Sites_in_Africa                Aapravasi ghat   \n",
       "1  /wiki/List_of_World_Heritage_Sites_in_Africa                      Abu mena   \n",
       "2  /wiki/List_of_World_Heritage_Sites_in_Africa  Air and tnr natural reserves   \n",
       "3  /wiki/List_of_World_Heritage_Sites_in_Africa                         Aksum   \n",
       "4  /wiki/List_of_World_Heritage_Sites_in_Africa        Al qala of beni hammad   \n",
       "\n",
       "                                            location  \n",
       "0  Port Louis District, Mauritius.mw-parser-outpu...  \n",
       "1  Abusir, Egypt305028N 293947E / 30.84098N 29.66...  \n",
       "2             Arlit Department, Niger18N 9E / 18N 9E  \n",
       "3  Tigray Region, Ethiopia140749N 384307E / 14.13...  \n",
       "4  Maadid, Algeria354906N 44713E / 35.818440N 4.7...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131275b8f10049d3b578abba6c45a81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3809.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(data) - 1)):\n",
    "    place1 = data.iloc[i].place\n",
    "    place2 = data.iloc[i+1].place\n",
    "    if lev(place1, place2) < 3: # if the writing of the place differs of less than 2 characters\n",
    "        data.iloc[i+1].place = place1 # only the first version of the writing is kept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset = 'place', keep='first',inplace=True)\n",
    "data.sort_values('place', ascending=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1189, 1189)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that all names of places are unique\n",
    "len(data), len(data.place.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe \n",
    "data.to_csv('../data/unesco_sites_cleaned.csv', sep = ';', header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video search\n",
    "\n",
    "Get all the links resulting from the youtube requests. Refer to video-search.py for the python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import youtube_dl\n",
    "import csv\n",
    "import urllib\n",
    "import urllib.request\n",
    "import re\n",
    "import unidecode\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pytube\n",
    "import pandas as pd\n",
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = csv.reader(open('../data/unesco_sites_cleaned.csv','r'), delimiter=';')\n",
    "\n",
    "ydl_opts = {\n",
    "'format': 'bestaudio/best',\n",
    "'outtmpl': 'tmp/%(id)s.%(ext)s',\n",
    "'noplaylist': True,\n",
    "'quiet': True,\n",
    "'prefer_ffmpeg': True,\n",
    "'audioformat': 'wav',\n",
    "'forceduration':True\n",
    "}\n",
    "\n",
    "# create an empty dictionnary\n",
    "video_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# read line by line\n",
    "for row in inputfile:\n",
    "    \n",
    "    print(row)\n",
    "    # get second column (names of places)\n",
    "    place = row[1]\n",
    "    \n",
    "    # clean string : remove accents\n",
    "    place_clean1 = unidecode.unidecode(place)\n",
    "    # clean string : remove spaces\n",
    "    place_clean2 = place_clean1.replace(' ', '+')\n",
    "    \n",
    "    # add key words \n",
    "    search_words = place_clean2 + \"+drone\"\n",
    "    \n",
    "    # make a request in youtube, store the results in a list\n",
    "    results = []\n",
    "    html = urllib.request.urlopen(\"https://www.youtube.com/results?search_query=\" + search_words)\n",
    "    \n",
    "    # store the results\n",
    "    video_ids = re.findall(r\"watch\\?v=(\\S{11})\", html.read().decode())\n",
    "  \n",
    "    for video_id in tqdm(video_ids):\n",
    "        try:\n",
    "            ydl_opts = {'ignoreerrors': True}\n",
    "            with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "                myVideo = YouTube(\"https://www.youtube.com/watch?v=%s\" % video_id)\n",
    "                if (myVideo.streams.filter(res=\"2160p\") != None) :\n",
    "                    if not myVideo.age_restricted:\n",
    "                        dictMeta = ydl.extract_info(\"https://www.youtube.com/watch?v=%s\" % video_id, download=False)\n",
    "            \n",
    "                        video_dict.update({video_id : [place_clean1, dictMeta['duration'], dictMeta['title'], dictMeta['upload_date']]})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"ERROR Catched and Passed\", e)\n",
    "            pass \n",
    "        \n",
    "    \n",
    "video_df = pd.DataFrame.from_dict(video_dict, orient='index', columns=['place', 'duration', 'title', 'date'])\n",
    "\n",
    "video_df.to_csv('../data/video_info.csv', index_label = 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screenshots\n",
    "\n",
    "Select randomly 10 places, from which select 10 random videos, and take 3 screenshots for each video without downloading it for analysis purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import youtube_dl\n",
    "import numpy\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/video_info.csv', delimiter=',')\n",
    "dict_sample = {}\n",
    "places_sel = random.sample(df.place.unique().tolist(), 10)\n",
    "for place in places_sel:\n",
    "    ids = df.loc[df['place'] == place].id.tolist()\n",
    "    ids_sample = random.sample(ids, 10)\n",
    "    dict_sample.update({place : ids_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_sample.items():\n",
    "    \n",
    "    place = key\n",
    "    for youtube_id in value:\n",
    "        print(youtube_id)\n",
    "        video_url = 'https://www.youtube.com/watch?v=%s' % youtube_id  #The Youtube URL\n",
    "        ydl_opts = {}\n",
    "        ydl = youtube_dl.YoutubeDL(ydl_opts)\n",
    "        info_dict = ydl.extract_info(video_url, download=False)\n",
    "\n",
    "        formats = info_dict.get('formats',None)\n",
    "\n",
    "        f = formats[-1]\n",
    "        url = f.get('url',None)\n",
    "        cap = cv2.VideoCapture(url)\n",
    "\n",
    "        x = 1\n",
    "        count = info_dict['duration'] / 4\n",
    "\n",
    "        while x < 4 :\n",
    "\n",
    "            for i in range(3):\n",
    "\n",
    "                time = x * info_dict['duration'] + ( 10 * i ) # each time, take 3 shots at 10 sec intervals\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                filename = \"../data/screenshots/%s-%d.png\" % (youtube_id, time)\n",
    "\n",
    "                cv2.imwrite(filename, frame)\n",
    "                count += int(info_dict['duration'] / 4)\n",
    "                cap.set(1,count)\n",
    "\n",
    "            x += 1\n",
    "\n",
    "        cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
